Goal
Generate Python code for a chatbot memory system using MongoDB Atlas and Azure OpenAI's text-embedding-3-large model (3072 dimensions).   

Environment
Replit (utilize Replit Secrets for environment variables).   

Language
Python 3.9+    

Core Libraries
pymongo (for synchronous MongoDB interaction)    
azure-openai    
python-dotenv (for loading secrets/env vars)    
tiktoken (for token counting if implementing summarization or context window limits)    
Configuration
Load environment variables from Replit Secrets using python-dotenv.   

Required Environment Variables (ensure code expects these):

MONGODB_ATLAS_URI: Full MongoDB Atlas connection string.   
AZURE_OPENAI_API_KEY: API key for Azure OpenAI resource.   
AZURE_OPENAI_ENDPOINT: Endpoint URL for Azure OpenAI resource.   
AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME: Deployment name for the Azure OpenAI 'text-embedding-3-large' model.   
AZURE_OPENAI_CHAT_DEPLOYMENT_NAME: Deployment name for Azure OpenAI chat model (if used for extraction/rewriting).   
OPENROUTER_API_KEY: API key if using OpenRouter instead of Azure for chat models.   
MongoDB Atlas Setup
Establish connection to MongoDB Atlas using MONGODB_ATLAS_URI.   
Use a specific database name (e.g., 'chatbot_memory_large').   
Define collection objects for:
'chat_sessions': Stores individual chat conversations.   
'user_profiles': Stores persistent user information.   
Collection Schemas (Conceptual - implement using Python classes or dict structures):

chat_sessions:
_id: ObjectId (auto-generated)
session_id: String (unique identifier for the chat session)
user_id: String (identifier for the user)
created_at: DateTime
updated_at: DateTime
message_history: Array of Objects:
role: String ("user" or "assistant")
content: String (the message text)
timestamp: DateTime    
embedding: Array of Floats (3072 dimensions, vector embedding of 'content', generated via Azure OpenAI text-embedding-3-large) - This field requires an Atlas Vector Search index configured for 3072 dimensions.    
metadata: Object (e.g., message_id)    
user_profiles:
_id: ObjectId (auto-generated)
user_id: String (unique identifier for the user, should be indexed for standard queries)
created_at: DateTime
updated_at: DateTime
facts: Object (structured key-value pairs, e.g., {"name": "Alice", "location": "CA", "interests": ["hiking", "AI"]}) - Specific fact fields used for filtering should be indexed.    
preferences_embeddings: Array of Objects:
text: String (original text representing a preference)
embedding: Array of Floats (3072 dimensions, vector embedding of 'text', generated via Azure OpenAI text-embedding-3-large) - This field requires an Atlas Vector Search index configured for 3072 dimensions.    
source_message_id: String (link back to chat message)    
timestamp: DateTime    
Azure OpenAI Setup
Initialize the Azure OpenAI client using environment variables:   

api_key=AZURE_OPENAI_API_KEY
azure_endpoint=AZURE_OPENAI_ENDPOINT
api_version (use a recent stable version, e.g., "2024-02-01" or later)
Core Functionality Implementation
Implement the following functions within a Python class (e.g., ChatMemoryManager):

__init__(self):
Load environment variables using dotenv.load_dotenv().   
Initialize MongoDB client (pymongo.MongoClient) and get database/collection objects. Handle connection errors.   
Initialize Azure OpenAI client (azure.openai.AzureOpenAI). Handle initialization errors.   
Store the embedding deployment name (AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME) from environment variables.   
_get_embedding(self, text: str) -> list[float]:
Internal helper function to generate embeddings using 'text-embedding-3-large'.   
Takes text input.   
Calls the Azure OpenAI client's embeddings.create method using the stored embedding deployment name.   
Crucially, ensure the request does NOT specify the 'dimensions' parameter, so it defaults to the full 3072 dimensions of text-embedding-3-large.   
Implement try-except blocks to handle potential API errors (e.g., openai.APIError, openai.RateLimitError) and return None or raise an exception.   
Returns the 3072-dimension embedding vector (list of floats).   
The embedding model deployed in Azure MUST be 'text-embedding-3-large' and the Atlas Vector Search index MUST be configured for 3072 dimensions.    
add_message(self, session_id: str, user_id: str, role: str, content: str) -> bool:
Generates a 3072-dimension embedding for the content using _get_embedding. If embedding fails, log an error and return False.   
Creates a message dictionary including role, content, timestamp (use datetime.utcnow()), and the generated 3072-dimension embedding.   
Uses update_one with upsert=True on the chat_sessions collection, filtering by session_id and user_id.   
Use $push to add the message dictionary to the message_history array.   
Use $set to update updated_at.   
Use $setOnInsert to set created_at and user_id only when creating a new session document.   
Return True on success, False on database error (include try-except).   
retrieve_short_term_memory(self, session_id: str, user_id: str, query_text: str, last_n: int = 10, vector_search_limit: int = 5) -> list[dict]:
Retrieves the most recent last_n messages chronologically from the specified chat session (session_id, user_id).   
Generates a 3072-dimension embedding for the query_text using _get_embedding. If embedding fails, return only the last_n messages.   
Performs an Atlas Vector Search query using an aggregation pipeline on chat_sessions.
Stage 1: $match to filter for the specific session_id and user_id.   
Stage 2: $vectorSearch stage:
index: Name of the Atlas Vector Search index created on chat_sessions (e.g., 'idx_message_embedding_large').   
path: "message_history.embedding".   
queryVector: The 3072-dimension embedding generated from query_text.   
numCandidates: Set higher than limit (e.g., 50 or 100) for ANN accuracy.   
limit: vector_search_limit.   
Ensure the index 'idx_message_embedding_large' is configured for 3072 dimensions and the correct similarity metric (e.g., cosine).    
  
Stage 3: $project to reshape results if needed (e.g., extract relevant messages and scores).   
  
Fetches the vector search results.   
Combines the last_n messages and vector search results, ensuring no duplicates. Prioritize relevance.   
Returns the consolidated list of message dictionaries.   
retrieve_long_term_memory(self, user_id: str, query_text: str, fact_filters: dict = None, vector_search_limit: int = 5) -> dict:
Generates a 3072-dimension embedding for the query_text using _get_embedding. If embedding fails, return empty results.   
Constructs an aggregation pipeline for the user_profiles collection.
Stage 1: $match stage to filter by user_id. If fact_filters are provided (e.g., {"facts.location": "CA"}), include these in the $match query.   
Stage 2: $vectorSearch stage:
index: Name of the Atlas Vector Search index created on user_profiles (e.g., 'idx_preference_embedding_large').   
path: "preferences_embeddings.embedding".   
queryVector: The 3072-dimension embedding generated from query_text.   
numCandidates: Set higher than limit (e.g., 50 or 100).   
limit: vector_search_limit.   
Ensure the index 'idx_preference_embedding_large' is configured for 3072 dimensions and the correct similarity metric.    
Include a filter within $vectorSearch itself if the index supports it and it's more efficient than pre-filtering with $match for the non-user_id fact filters.    
  
Stage 3: $project to extract facts and relevant preferences_embeddings (including scores via $meta: "vectorSearchScore").   
  
Executes the aggregation pipeline.   
Processes the result (should be one document or none).   
Returns a dictionary like: {"matching_facts": {...}, "similar_preferences": [...]} or an empty dictionary if no profile/matches found.   
extract_structured_info(self, text: str) -> dict:
Requires an Azure OpenAI Chat deployment or OpenRouter setup.   
Initialize the appropriate client (Azure Chat or OpenRouter).   
Define a Pydantic model or JSON schema representing the desired structured information (e.g., UserProfileInfo with fields like name: Optional[str], location: Optional[str], interests: Optional[list[str]], explicit_preferences: Optional[list[str]]).   
Construct a prompt instructing the LLM to extract information from the text according to the schema. Use function calling/tool calling features if available on the LLM for reliable structured output.   
Make the API call to the chat model.   
Parse the response to get the structured data. Handle potential parsing errors or cases where the LLM couldn't extract information.   
Return the extracted dictionary.   
update_user_profile(self, user_id: str, info: dict) -> bool:
Takes structured info (likely from extract_structured_info).   
Prepare update operations for the user_profiles collection based on info.   
Use $set to update simple fields in the facts object (e.g., facts.name, facts.location). Use dot notation.   
Use $addToSet for list-based facts like facts.interests to avoid duplicates.   
For preferences (e.g., info['explicit_preferences']), iterate through them:
Generate an embedding for each preference text using _get_embedding.   
Create a preference embedding object: {"text": pref_text, "embedding": pref_embedding, "timestamp": datetime.utcnow()}.   
Use $push (or $addToSet if duplicate preference texts should be strictly avoided based on text) to add these objects to the preferences_embeddings array.   
  
Include $set: {"updated_at": datetime.utcnow()} in the update.   
Perform an update_one operation on user_profiles filtering by user_id. Use upsert=True if a profile might not exist yet (setting created_at and user_id on insert).   
Return True on success, False on database error.   
rewrite_query(self, chat_history: list[dict], follow_up_query: str) -> str:
Requires an Azure OpenAI Chat deployment or OpenRouter setup.   
Format the chat_history (e.g., last few turns) and follow_up_query into a suitable prompt for the LLM.   
The prompt should instruct the LLM to generate a standalone query based on the follow_up_query that incorporates necessary context from the chat_history. Example prompt structure can be adapted from query transformation techniques.   
Make the API call to the chat model.   
Extract the rewritten query from the LLM's response. Handle errors.   
Return the rewritten query string. If rewriting fails, return the original follow_up_query.   
Streaming Compatibility Considerations
The memory retrieval functions (retrieve_short_term_memory, retrieve_long_term_memory) and embedding generation (_get_embedding) should be executed before the final call to the generative LLM that produces the user-facing response.   
These memory operations add latency before generation starts, but they do not inherently block the ability to stream the final response from the generative LLM.   
To minimize user-perceived latency:
Ensure MongoDB indexes (vector and standard) are correctly configured and utilized.   
Consider performing memory updates (like add_message and update_user_profile) asynchronously after the response stream to the user has begun. This prevents database write latency from delaying the start of the response.   
If using query rewriting, use a fast LLM for that specific task to reduce its latency impact.   
  
Error Handling & Logging
Include basic try...except blocks around database operations and API calls.   
Use Python's logging module to log informational messages (e.g., adding message, retrieving memory) and errors (e.g., API failures, DB connection issues). Configure a basic logger.   
Final Output
Provide the complete Python code, including necessary imports, the ChatMemoryManager class definition, and all implemented methods.   
Ensure the code is well-commented, explaining the purpose of key sections and logic, especially the dimensionality requirements (3072).   
Adhere to PEP 8 Python style guidelines.   
