We need to fix the message truncation and missing metadata issues. The .replit file has been manually updated to use the uvicorn worker (-k uvicorn.workers.UvicornWorker), which is necessary for async operations.

Now, please perform the following steps to correctly implement asynchronous streaming using httpx:

Ensure Dependencies: Run the following command in the Shell to make sure httpx, uvicorn, and Flask's async extras are installed and registered in the project dependencies:

Bash

uv pip install httpx uvicorn "Flask[async]"
Update app.py:

Import httpx: Add import httpx at the top.

Replace Chat Endpoint: Replace the entire chat() endpoint function (from @app.route('/chat', methods=['POST']) down to the return Response(...) line) with the following async version using httpx:

Python

# === CODE TO REPLACE THE ENTIRE chat() ENDPOINT IN app.py ===
@app.route('/chat', methods=['POST'])
async def chat(): # Make the endpoint async
    """
    Endpoint to handle chat messages and stream responses from OpenRouter using httpx
    """
    try:
        # --- Get request data (keep your existing logic here) ---
        data = request.get_json()
        user_message = data.get('message', '')
        model_id = data.get('model', defaultModels['1']) # Use a default if needed
        message_history = data.get('history', [])
        conversation_id = data.get('conversation_id', None)

        from models import Conversation, Message # Ensure models are imported

        # --- Determine OpenRouter Model ID (keep your existing logic) ---
        if model_id in OPENROUTER_MODELS:
            openrouter_model = OPENROUTER_MODELS[model_id]
        else:
            openrouter_model = model_id # Assume direct ID

        # --- Get API Key (keep your existing logic) ---
        api_key = os.environ.get('OPENROUTER_API_KEY')
        if not api_key:
            logger.error("OPENROUTER_API_KEY not found")
            abort(500, description="API key not configured")

        # --- Prepare Headers (keep your existing logic) ---
        headers = {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json',
            'HTTP-Referer': request.headers.get('Referer', 'https://gloriamundo.com') # Adjust Referer if needed
        }

        # --- Get/Create Conversation & Save User Message (keep your existing logic) ---
        conversation = None
        if conversation_id:
            conversation = Conversation.query.get(conversation_id)

        if not conversation:
            title = user_message[:50] + "..." if len(user_message) > 50 else user_message
            share_id = generate_share_id() # Ensure generate_share_id is defined
            conversation = Conversation(title=title, share_id=share_id)
            # Add user_id if user is logged in:
            # if current_user and current_user.is_authenticated:
            #    conversation.user_id = current_user.id
            db.session.add(conversation)
            db.session.commit()
            conversation_id = conversation.id

        user_db_message = Message(
            conversation_id=conversation.id, role='user', content=user_message
        )
        db.session.add(user_db_message)
        db.session.commit()

        # --- Prepare Message History (keep your existing logic including memory system if enabled) ---
        system_message = "..." # Add your system message back here
        messages = [{'role': 'system', 'content': system_message}]

        # Load history from DB or use provided history
        if message_history: # Use history from request if provided
            messages.extend(message_history)
        elif conversation_id: # Otherwise load from DB if conversation exists
             db_messages = Message.query.filter_by(conversation_id=conversation.id).order_by(Message.created_at).all()
             # Exclude the user message just added if it's somehow in db_messages already
             for msg in db_messages:
                  if msg.id != user_db_message.id: 
                      messages.append({'role': msg.role, 'content': msg.content})

        messages.append({'role': 'user', 'content': user_message}) # Add current user message

        # --- Enrich with memory if needed (keep your existing logic) ---
        if ENABLE_MEMORY_SYSTEM:
            try:
                memory_user_id = str(current_user.id) if current_user and current_user.is_authenticated else f"anonymous_{conversation_id}"
                messages = enrich_prompt_with_memory(
                     session_id=str(conversation_id), user_id=memory_user_id, 
                     user_message=user_message, conversation_history=messages
                )
            except Exception as e:
                 logger.error(f"Error enriching with memory: {e}")


        # --- Prepare Payload for OpenRouter ---
        payload = {
            'model': openrouter_model,
            'messages': messages,
            'stream': True
            # Add any other parameters like temperature if needed
        }
        logger.debug(f"Sending request to OpenRouter with model: {openrouter_model}")

        # --- Define the ASYNC Generator using httpx ---
        async def generate():
            assistant_response_content = [] 
            final_prompt_tokens = None
            final_completion_tokens = None
            final_model_id_used = None
            assistant_message_id = None 

            try:
                async with httpx.AsyncClient(timeout=300.0) as client: 
                    async with client.stream('POST','https://openrouter.ai/api/v1/chat/completions', headers=headers, json=payload) as response:

                        if response.status_code != 200:
                            error_content = await response.aread() 
                            logger.error(f"OpenRouter API error: {response.status_code} - {error_content.decode()}")
                            yield f"data: {json.dumps({'type': 'error', 'error': f'API Error: {response.status_code}'})}\n\n"
                            return

                        async for line in response.aiter_lines():
                            if line:
                                if line.strip() == '': continue 

                                if line.startswith('data: '):
                                    sse_data = line[6:].strip()
                                    if sse_data == '[DONE]': continue 

                                    try:
                                        if not sse_data: continue
                                        json_data = json.loads(sse_data)

                                        content_chunk = None
                                        if 'choices' in json_data and len(json_data['choices']) > 0:
                                            choice = json_data['choices'][0]
                                            if 'delta' in choice and choice['delta'].get('content') is not None:
                                                content_chunk = choice['delta']['content']

                                        if content_chunk:
                                            assistant_response_content.append(content_chunk)
                                            yield f"data: {json.dumps({'type': 'content', 'content': content_chunk, 'conversation_id': conversation_id})}\n\n"

                                        if 'usage' in json_data and json_data['usage']: 
                                            usage = json_data['usage']
                                            final_prompt_tokens = usage.get('prompt_tokens')
                                            final_completion_tokens = usage.get('completion_tokens')
                                            logger.debug(f"Found usage data: P:{final_prompt_tokens} C:{final_completion_tokens}")

                                        if 'model' in json_data and json_data['model']:
                                            final_model_id_used = json_data.get('model')
                                            logger.debug(f"Found model used: {final_model_id_used}")

                                    except json.JSONDecodeError as e:
                                        logger.error(f"JSON decode error: {e} on line content: {sse_data}")
                                        yield f"data: {json.dumps({'type': 'error', 'error': 'JSON parsing error'})}\n\n"
                                        return 

                # --- Stream processing finished ---
                full_response_text = ''.join(assistant_response_content)

                if full_response_text: 
                    try:
                        # Need to re-import or have Message available
                        from models import Message 
                        assistant_db_message = Message(
                            conversation_id=conversation_id, role='assistant', content=full_response_text,
                            model=model_id, model_id_used=final_model_id_used, 
                            prompt_tokens=final_prompt_tokens, completion_tokens=final_completion_tokens,
                            rating=None 
                        )
                        db.session.add(assistant_db_message)
                        db.session.commit()
                        assistant_message_id = assistant_db_message.id 
                        logger.info(f"Saved assistant message {assistant_message_id} with metadata.")

                        # Save to memory system if enabled
                        if ENABLE_MEMORY_SYSTEM:
                             try:
                                 memory_user_id = str(current_user.id) if current_user and current_user.is_authenticated else f"anonymous_{conversation_id}"
                                 # Ensure save_message_with_memory is available
                                 save_message_with_memory(
                                     session_id=str(conversation_id), user_id=memory_user_id, 
                                     role='assistant', content=full_response_text
                                 )
                             except Exception as e:
                                 logger.error(f"Error saving assistant message to memory: {e}")

                        logger.info(f"==> Preparing to yield METADATA for message {assistant_message_id}")
                        yield f"data: {json.dumps({'type': 'metadata', 'metadata': {'id': assistant_message_id, 'model_id_used': final_model_id_used, 'prompt_tokens': final_prompt_tokens, 'completion_tokens': final_completion_tokens}})}\n\n"
                        logger.info(f"==> SUCCESSFULLY yielded METADATA for message {assistant_message_id}")

                    except Exception as db_error:
                        logger.exception("Error saving assistant message or metadata to DB")
                        db.session.rollback()
                        yield f"data: {json.dumps({'type': 'error', 'error': 'Error saving message to database'})}\n\n"

                logger.info("==> Preparing to yield DONE event")
                yield f"data: {json.dumps({'type': 'done', 'done': True, 'conversation_id': conversation_id})}\n\n"
                logger.info("==> SUCCESSFULLY yielded DONE event. Stream generation complete.")

            except httpx.RequestError as e:
                 logger.exception(f"httpx request error: {e}")
                 yield f"data: {json.dumps({'type': 'error', 'error': f'Connection error: {e}'})}\n\n"
            except Exception as e:
                logger.exception("Error during async stream generation")
                yield f"data: {json.dumps({'type': 'error', 'error': f'Stream Error: {str(e)}'})}\n\n"

        # --- Return the Response object wrapping the generator ---
        return Response(generate(), content_type='text/event-stream') # No stream_with_context needed

    except Exception as e:
        logger.exception("Error in chat endpoint setup")
        # Use abort for standard Flask error handling
        abort(500, description=str(e)) 
# === END OF REPLACEMENT CODE FOR chat() ENDPOINT ===
Update static/js/script.js:

Replace the stream processing logic within sendMessageToBackend (specifically the processChunks function and its call) with the version designed for the async backend's event types (content, metadata, done, error).

JavaScript

// === CODE TO REPLACE STREAM PROCESSING LOGIC IN script.js (within sendMessageToBackend) ===
        // Setup event source for streaming using fetch response body reader
        const reader = response.body.getReader();
        const decoder = new TextDecoder();
        let buffer = '';
        let responseText = ''; // Accumulates the text content

        // Remove typing indicator BEFORE adding the empty message
        if (typingIndicator && typingIndicator.parentNode) {
            chatMessages.removeChild(typingIndicator);
        }

        // Add empty assistant message element to append to
        const assistantMessageElement = addMessage('', 'assistant'); 
        // Ensure addMessage returns the created element
        if (!assistantMessageElement) {
            console.error("Failed to create assistant message element.");
            return; // Stop if element creation failed
        }
        const messageContent = assistantMessageElement.querySelector('.message-content');
        if (!messageContent) {
             console.error("Failed to find message content element.");
             return; // Stop if element structure is wrong
        }

        // --- Define function to process stream chunks ---
        function processChunks() {
            return reader.read().then(({ done, value }) => {
                if (done) {
                    console.log("Reader finished (stream closed by server)."); 
                    // Check if we ever received a 'done' event type from the app
                    if (!assistantMessageElement.dataset.streamCompleted) {
                         console.warn("Stream closed before 'done' event received from application.");
                         // Handle potential incomplete message: maybe add final text to history?
                         if (responseText && (!messageHistory.length || messageHistory[messageHistory.length - 1]?.role !== 'assistant')) {
                             messageHistory.push({ role: 'assistant', content: responseText });
                             console.log("Added potentially incomplete assistant response to JS history on stream close.");
                         }
                    }
                    return; // Stop processing
                }

                buffer += decoder.decode(value, { stream: true });
                const potentialMessages = buffer.split('\n\n');
                buffer = potentialMessages.pop(); 

                for (const message of potentialMessages) {
                    if (message.trim() === '') continue;

                    if (message.startsWith('data: ')) {
                        const data = message.substring(6).trim(); 
                        if (!data) continue; 

                        try {
                            const parsedData = JSON.parse(data);
                            console.log("==> Received SSE Data:", parsedData); 

                            // --- Handle different data types ---
                            if (parsedData.type === 'error' || parsedData.error) {
                                const errorMsg = parsedData.error || 'Unknown error occurred';
                                messageContent.innerHTML = `<span class="error">Error: ${errorMsg}</span>`;
                                console.error("Received error from backend:", errorMsg);
                                assistantMessageElement.dataset.streamCompleted = 'true'; // Mark as completed (with error)
                                return; // Stop processing on error

                            } else if (parsedData.type === 'content') {
                                console.log("==> Processing type: content"); 
                                if (parsedData.content) {
                                    responseText += parsedData.content;
                                    // Use your existing formatMessage function
                                    messageContent.innerHTML = formatMessage(responseText); 
                                    chatMessages.scrollTop = chatMessages.scrollHeight;
                                }
                                if (parsedData.conversation_id && !currentConversationId) {
                                    currentConversationId = parsedData.conversation_id;
                                    console.log(`Setting conversation ID: ${currentConversationId}`);
                                    // Store conversation ID on the message element if needed
                                    // assistantMessageElement.dataset.conversationId = currentConversationId; 
                                }

                            } else if (parsedData.type === 'metadata') {
                                console.log("==> Processing type: metadata"); 
                                if (parsedData.metadata) {
                                    const meta = parsedData.metadata;
                                    assistantMessageElement.dataset.messageId = meta.id;

                                    const messageWrapper = assistantMessageElement.querySelector('.message-wrapper');
                                    if (messageWrapper) { 
                                        let metadataContainer = messageWrapper.querySelector('.message-metadata');
                                        if (!metadataContainer) {
                                            metadataContainer = document.createElement('div');
                                            metadataContainer.className = 'message-metadata';
                                            const actionsContainer = messageWrapper.querySelector('.message-actions');
                                            if (actionsContainer) {
                                                messageWrapper.insertBefore(metadataContainer, actionsContainer);
                                            } else {
                                                messageWrapper.appendChild(metadataContainer); 
                                            }
                                        }

                                        let metadataText = '';
                                        const modelName = meta.model_id_used ? formatModelName(meta.model_id_used) : 'N/A';
                                        metadataText += `Model: ${modelName}`;

                                        if (meta.prompt_tokens !== null && meta.completion_tokens !== null) {
                                             metadataText += ` · Tokens: ${meta.prompt_tokens} prompt + ${meta.completion_tokens} completion`;
                                        }
                                        metadataContainer.textContent = metadataText;
                                    }
                                    // Update action buttons now that we have the final message ID
                                    updateActionButtonsWithMessageId(assistantMessageElement, meta.id);
                                }

                            } else if (parsedData.type === 'done') {
                                console.log("==> Processing type: done"); 
                                assistantMessageElement.dataset.streamCompleted = 'true'; // Mark completion

                                // Ensure the final response text is added to JS history correctly
                                if (responseText && (!messageHistory.length || messageHistory[messageHistory.length - 1]?.role !== 'assistant')) {
                                    messageHistory.push({ role: 'assistant', content: responseText });
                                    console.log("Added final assistant response to JS history.");
                                } else if (responseText && messageHistory.length > 0 && messageHistory[messageHistory.length - 1]?.role === 'assistant' && messageHistory[messageHistory.length - 1]?.content !== responseText) {
                                     // Optionally update if the final text differs somehow, though unlikely now
                                     // messageHistory[messageHistory.length - 1].content = responseText;
                                     // console.log("Updated final assistant response in history.");
                                }

                                // Re-enable input, etc. 
                                // messageInput.disabled = false;
                                // sendButton.disabled = false;

                                return; // Exit the processing loop
                            } else {
                                 console.warn("==> Received unknown data type:", parsedData.type, parsedData); 
                            }

                        } catch (error) {
                            console.error('Error parsing SSE data JSON:', error, data);
                            // Stop processing on parsing error
                            assistantMessageElement.dataset.streamCompleted = 'true'; 
                            messageContent.innerHTML = `<span class="error">Error processing response data.</span>`;
                            return; 
                        }
                    } else {
                        // Log non-data lines if needed for debugging
                        // console.log("Received non-data line:", message); 
                    }
                } // End of loop processing complete messages

                // Continue reading from the stream
                return processChunks();
            }); // End of reader.read().then()
        } // End of processChunks function definition

        // Helper function (ensure this is defined or integrated - should be outside processChunks)
        function updateActionButtonsWithMessageId(messageElement, messageId) {
            if (!messageElement || !messageId) return; 
            const upvoteBtn = messageElement.querySelector('.upvote-btn');
            const downvoteBtn = messageElement.querySelector('.downvote-btn');
            const copyBtn = messageElement.querySelector('.copy-btn');
            // Share button likely still uses conversation ID, check its logic
            if (upvoteBtn) upvoteBtn.dataset.messageId = messageId;
            if (downvoteBtn) downvoteBtn.dataset.messageId = messageId;
            if (copyBtn) copyBtn.dataset.messageId = messageId; 
        }

        // Start the stream processing
        return processChunks();
// === END OF REPLACEMENT CODE FOR STREAM PROCESSING LOGIC ===